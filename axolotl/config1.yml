
# start:
# cd /workspace/axolotl
# axolotl train /workspace/FineTuningTrial/axolotl/config1.yml

# axolotl_config.yaml
# base_model: meta-llama/Llama-3-8B
# model_type: AutoModelForCausalLM
# tokenizer_type: AutoTokenizer

base_model: NousResearch/Llama-3.2-1B
adapter: lora

micro_batch_size: 2
num_epochs: 3
learning_rate: 0.0003

#max_steps = (num_samples / batch_size) * num_epochs
max_steps: 20
eval_strategy : no


# Load model in 4-bit precision to optimize memory usage
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# Dataset configuration
#datasets:
#  - path: /workspace/FineTuningTrial/learning_json/llm_friendly_dataset.jsonl
#    type: text

pretraining_dataset:
  - path: json
    data_files:
      - /workspace/FineTuningTrial/learning_json/llm_friendly_dataset.jsonl

# Training parameters
sequence_len: 2048
val_set_size: 0.05  # 5% of data used for validation
output_dir: ./outputs/llama-8b-finetune

# LoRA (Low-Rank Adaptation) parameters for efficient fine-tuning
adapter: lora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj

# Training hyperparameters
micro_batch_size: 2
gradient_accumulation_steps: 4
num_epochs: 3
learning_rate: 0.0002
lr_scheduler: cosine
warmup_steps: 100

# Mixed precision training for faster and memory-efficient training
bf16: true

# Gradient checkpointing to save memory
gradient_checkpointing: true

# Logging and monitoring
wandb_project: your_wandb_project_name
wandb_entity: your_wandb_entity_name
logging_steps: 10

# Additional configurations
train_on_inputs: false
group_by_length: false
