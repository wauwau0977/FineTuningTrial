The purpose of dividing the total number of seconds in the date range by the number of data points would be to determine the *maximum* number of seconds allowed per data point to ensure a reasonable level of granularity. This calculation effectively determines the interval size.  By limiting the number of seconds per data point, the algorithm is trying to ensure that the data is not excessively condensed.  

Specifically, this calculation answers the question: "How much time (in seconds) can we allocate to each data point before the time series becomes too granular?" The result of this calculation then likely maps to a predefined set of intervals (e.g., "1d", "1h", "4h") to provide a standardized time resolution. The logic is aiming to dynamically adjust the interval based on data density.  If there are many data points within a short time period, the interval will be smaller (e.g. 1h). If there are fewer, the interval will be larger (e.g., 1d).