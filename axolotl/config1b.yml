
# start:
# cd /workspace/axolotl
# axolotl train /workspace/FineTuningTrial/axolotl/config1.yml

# axolotl_config.yaml
# base_model: meta-llama/Llama-3-8B
# model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

base_model: NousResearch/Hermes-3-Llama-3.2-3B
adapter: lora

micro_batch_size: 2
num_epochs: 30
learning_rate: 0.0003

#max_steps = (num_samples / batch_size) * num_epochs
max_steps: 10000
eval_strategy: "no"

save_embedding_layers: true


special_tokens:
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
  pad_token: "[PAD]"


# Load model in 4-bit precision to optimize memory usage
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# Dataset configuration
#datasets:
#  - path: /workspace/FineTuningTrial/learning_json/llm_friendly_dataset.jsonl
#    type: text

pretraining_dataset:
  - path: json
    data_files:
      - /workspace/data/FineTuningTrial/learning_json/pretrain_dataset.jsonl

# Training parameters
sequence_len: 2048
val_set_size: 0.05  # 5% of data used for validation
output_dir: ./outputs/llama-8b-finetune

# LoRA (Low-Rank Adaptation) parameters for efficient fine-tuning
adapter: lora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj

lora_modules_to_save:
  - embed_tokens
  - lm_head


# Training hyperparameters
micro_batch_size: 2
gradient_accumulation_steps: 4
num_epochs: 3
learning_rate: 0.0002
lr_scheduler: cosine
warmup_steps: 100

# Mixed precision training for faster and memory-efficient training
bf16: true

# Gradient checkpointing to save memory
gradient_checkpointing: true

# Additional configurations
train_on_inputs: false
group_by_length: false
